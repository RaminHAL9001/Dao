UNSOLVED:

* REGRESSION: Creating, opening, and saving idea files no longer seems to work when using the
	built-in functions "newDB", "openDB", and "saveDB". However, the "test1.idea" file is loaded
	when it is sepcified in the command line arguments, so the problem is not with
	'initRuntimeFiles'.
	--
	I am transitioning away from using the "Dao.Document" functions to using the "Dao.Files"
	functions. "Dao.Document" was one of the earliest code I wrote in this iteration of the Dao
	system, it is no longer useful given all the new features I have added over the past year. The
	new method of loading and saving files will be more transparent. Files are loaded on demand
	(this is kind of a given, since Haskell tends to read files lazily), and written when explicitly
	told to be written. Any file loaded is loaded only once, and reference counting is used to keep
	track of when it is OK to remove a loaded file from the table (close it).

* Create a parser to load a configuration file with rules dictating which files and programs in
	which directories are permitted to be loaded by running Dao programs. These rules must be
	consulted every time a file is to be loaded, whether it is imported or loaded with the
	"openDB()" built-in function. The configuration must also have the ability dictate which
	built-in function sets are permitted to be used by each Dao program. I have already marked
	several places in the source code that will use this a feature once it has has been implemented.

* Change the instantiation of UStr into the Binary monad to use a minimum number of bytes to
	indicate the length of the string. Right now, every UStr is preceeded a 64-bit (8 byte) number.
	However, the vast majority of all strings saved to "idea" files will most certainly be less than
	128 characters long. Especially the UStr's used for the paths in Tree objects. I would like
	UStr's to now be stored using an adaptive length prefix. The bits preceeding a UStr's contents
	will have a format like so:
		7 6543210
		---------
		1 XXXXXXX
		1 XXXXXXX
		1 XXXXXXX
		...
		0 XXXXXXX
	If the highest-order bit is a one, it indicates there are more bytes to follow. If the highest
	order bit is 0, then there are no more bytes. There will be a maximum of 9 bytes. The 7
	lower-order bits will be concatenated in big-endian order to form the length value for the
	string. By this method, most all strings will have a length prefix of only one or two bytes.

* Parser needs to return better error messages.
	--
	In progress. Implemented a "labelParse" function which essentially replaces all failed parses
	with a single message indicating why the parse failed. But I have not done enough to
	pretty-print these messages.

* Consider making all errors caused during script evaluation into "OTree" values.

* Once you are sure the Dao language isn't going to change much more, make a more efficient parser
	from the "Text.ParserCombinators.Parsec" library.

* We need more built-in functions to do cool stuff!

* Document everything, and I mean everything.

* Create some awesome example programs.

----------------------------------------------------------------------------------------------------
SOLVED:

* Modify the 'ExecUnit' to contain built-in functions decalred in the top-level of a Dao program.
	Presently, these objects are stored in the 'staticHeap', which are not cached, and are required
	to be called by the "call" keyword using the reference "${topLevelFunction}".
	-- Done.

* The matching predicate needs to be retrieved from a declaration in the program. Right now, the
	matching predicate is hard-wired to use the (==) operator.
	-- Done.

* Tokenizing input strings needs to be done by a function retrieved from a declaration in the
	program, or by a function written in the program itself. Right now, the tokenizing function is
	hard-wired to use the 'Dao.Pattern.tokens' function.
	-- Done.

* Modify the runtime and language specification to have a "require" directive. The "require"
	directive will ask the Dao.Runtime about the properties of the Runtime itself. This will make it
	easier to automatically reject programs that are using a version of the Dao library with which
	that program is incompatible. So, for example, if a programmer has used the Dao library to write
	their own Dao system that has Voice Pattern objects marshalled and stored into a OBinary
	objects, and these Voice Pattern objects can be manipulated by Dao rules at runtime, then the
	programmer can can hard-code a "My Voice Patterns" property into their version of the Dao runtime
	that they have constructed using the Dao library. Programs can then declare:
		require "My Voice Patterns"
	If they then send their Dao code to some friends using the standard version of Dao those
	friends try to load the program, the Dao interpreter will report an error at loading time.
	--
	Done. Actually, I eliminated the "import" directive. Now there is simply an 'Attribute'
	directive, and "import" and "require" statements, as well as the directives for specifying a
	tokenizing function ("string.tokenizer") and a matching predicate ("string.compare"). I had to
	overhaul the 'Dao.Evaluator.programFromSource' function, but now the code for that function is
	cleaner and will be much easier to make changes to in the future, if necessary.

* REGRESSION: global variables are not being defined properly.
	--
	Solved: the solution was elegant, and lead to a host of simplifications to the code:
	(1) Added a new data value called 'GlobalRef' to the 'ObjectExpr' data type.
	(2) Changed the parser for 'ORef' literal expressions to require two dollar signs, for example:
		${this.is.a.globalRef} = $${this.is.a.reference.literal};
	(3) Changed the parser to return 'GlobalRef's or 'Literal (ORef val)'s depending on the number
		of dollar signs occuring before the expression.
	(4) Modified the pretty pritner to reflect these changes.
	(5) Modified the "Dao.Object.Binary" serialization for the 'ObjectExpr' data type.
	Now, references with two dollar-signs are literal references, and references with one
	dollar-sign is triggers a global lookup. This allowed me to make the following simplifications:
	(1) Eliminated the 'evalObjectDeref' function, now whether an object expression is dereferenced
		during it's evaluation depends simply on whether it is a Literal or a GlobalRef.
	(2) Eliminated 'Check' functions that used 'evalObjectDeref', they are no longer necessary.
	(3) Changed any built-in functions using the 'Check' functions that were using 'evalObjectDefef'
		such that they now check parameter arguments using ordinary Haskell pattern matching and
		"case" statements.
	(4) Simplified the 'Dao.Object.Data' module to make use of the PredicateIO monad, rather than
		the ContErrT monad. Now, Haskell pattern matching is used exclusively for type checking all
		function calls during evaluation of Dao programs.

* Built-in functions need to return more helpful error message on type-check failure. At present the
	error reported is the error given by the Haskell language, related to lambda functions or case
	expression not having exauhustive pattern matching, which tells users nothing about why which
	type causd it to fail.
	--
	Solved: one of the solutions that resulted naturally from the simplification of the evalObject
	function thanks to the new GlobalRef ObjectExpr. Now, all type checking is done in the
	PredicateIO monad using only Haskell pattern matching and "case" statements. I can now create
	error messages at the boundary between the "PredicateIO" and "ExecScript" monads. This boundary
	is the "checkToExecScript" function. I modified this function to require a function name and an
	Object indicating the parameter arugments passed to this function. If a PredicateIO fails due to
	bad pattern matching or a non-exhuastive "case" statement, then "checkToExecScript" will create
	an error indicating the name of the function that failed, and the argument parameters that
	caused the failure. This error is then propagated upwards by the ContErrT continuation
	mechanism.

* REGRESSSION: When I wrote a rule into my Dao test program that contained many "else if"
	expressions, this one rule seems to cause the Dao system to freeze on any rule, not just the
	rule with all the "else if" statements. Any rule causes Dao to freeze up entirely as long as
	this one rule exists.
	--
	It had nothing to do with "else if" statements, it had to do with a bad Pattern expression.
	Before now I had never tested a malformed pattern with the pattern parser. It turns out the
	parser went into an infinite loop when it saw a "$" preceeding a non-special character. Now,
	these characters are ignored and parsed as ordinary characters. Only the "$?", "$*", and "$$"
	constructs are parsed as special units of the patterns.

* Experiment with using Dao.Combination to check function input arguments.
	--
	Done, we now have a "PredicateIO" monad that extends "Dao.Combination", and catches IO errors
	caused by non-exhaustive Haskell-language patterns or case statements. It works perfectly,
	although I don't know how efficient it is. It could be as bad as the parser.

* REGRESSION: global lookups are not working as expected.
	--
	Solved: I stupidly copy-pasted the 'Dao.Object.Evaluator.staticDataLookup' function as, then
	re-named it to, 'currentDocumentLookup'. Probably because it has the same signature as
	'staticDataLookup', and probably with the intent of re-writing it to lookup information in the
	current document. But then I stupidly forgot to implement the currentDocumentLookup, that or I
	implemented but accidentally erased the implementation after copy-pasting. So staticDataLookup
	was occurring twice under two different functions.

* REGRESSION: the local variable stack is not replaced with an empty stack on a function call, so
	sub-functions can alter the values of local variables.
	--
	Re-named "pushExecStack" to "nestedExecStack", and created a new "pushExecStack" which creates a
	new MVar containing an empty stack, and uses Control.Monad.Reader.local to replace the existing
	"execStack" with the new MVar, then calls the inner function inside of "catchCEReturn" to behave
	like a C-style function call.

* REGRESSION: not all rules are executing after my last major code overhaul. The "hello" rule, which
	is the first in the program, is not exeuting anymore. It might be being over-written by a later
	"hello" rule. Correction: the regression seems to be somewhere in the global/local variable
	lookup.
	--
	I had removed the "let" language construct, and I had not created a suitable replacement
	parse. The solution was to create a new parser "localAssignExpr" that was placed in objectExpr
	in this order: [unaryOperator, localAssignExpr, equationExpr]
	The localAssignExpr returns a value of (AssignExpr LocalRef ObjectExpr).
	Local assignment expressions are parsed before equation expressions to give them higher
	prescedence, even though the equals operator is of a very low prescedence. Equations can still
	parse equals expressions, but they are handeled slightly differently

* REGRESSION: printing 'AssignExpr's does not work as expected.
	-- Solved.

* Make an "Cached" script type, that caches the (ExecScript ()) by storing the function along
	side it's abstract syntax tree. This will take up more memory, but useful thunks will not be
	de-allocated by the Haskell runtime as long as there is a reference to that function. That is
	the general idea, anyway. I am still not sure if that is what is really happening under the
	hood.
	--
	Done, created the 'CachedExec' and 'CXRef' types. CXRef is an MVar containing 'CachedExec' such
	that once the Abstract Syntax Tree (AST) value is evaluated to an "ExecScript" monadic type,
	this evaluation can be stored into the MVar, hopefully indicating to the Haskell Run-Time System
	that thunks associated with this monadic value should not be garbage collected.

* Modify ScriptExpr parser so 'AssignExpr's have a higher prescedence than 'Equation's to prevent
	all local assignment operations from being evaluated by the Equation function.
	-- Done. 

* Modify "with" statement to work on references, and references in other files.
	--
	Done. Now the "with" keyword can be followed by the following: (1) Object of type "ORef" to
	indicate modifications to a specific branch of working memory, (2), an Object of type "OString",
	where the string points to a file path indexed in the "documentList" of the Dao Runtime. If this
	path points to a "File" of type "DataFile", then the "with" block will modify the contents of
	that idea file. (3) An Object of type OPair (OString, ORef) which points to both a file and a
	branch in the tree of that file which the "with" block should modify.

* Go through Dao.Object.Evaluate and try to make updates to the ExecUnit state as few as possible,
	replacing the (ContErrT (ReaderT (MVar ExecUnit) IO) a) with (ContErrT (ReaderT ExecUnit IO) a)
	and moving the individual updatable fields of the ExecUnit structure into their own MVars.
	--
	Done. And I am still not sure if I want to have the 'ProgramTable' elemnts contain the
	'ExecUnit' values themselves, or have 'MVar's elemnts that containing 'ExecUnit's. Right now,
	the elements are 'MVar's, so there is some ability to modify the 'ExecUnit's at the
	"Dao.Runtime" level (where the 'programs::ProgramTable' data exists) without having to update
	the whole ProgramTable Data.Map.Map object every time you want to change the ExecUnit. But I am
	not sure if this is necessary, and the additional MVar's may just be creating an extraneous
	unboxing step.

* Modify the "dumpRules()" builtin function to return rules instead of OPair(OPattern p, OScript q).
	Break-up the Dao.Runtime.loadSource function into indivudual loading steps, that succede and
	return a value, or fail and return Nothing. Then compose the loadSource function with
	(sequence [step1, step2, step3] >>= return . msum)
	I did this, but I could not really compose 'loadSource' into three steps that could be executed
	by the sequence function because each type of file that was loaded was of a different type, a
	either a SourceCode object, a (Document DocData) object. So instead, I re-named the 'loadSource'
	function to the 'readFile' function, and split-off the source code parsing function to a
	separate function called 'loadSourceCode'. I also created the 'initSourceCode' and
	'initSourceIntoRuntime' functions to give the API more fine-grained control over loading source
	code.

* Make a errorPair function that throws takes a Object and String and returns the two paired
	together, and then clean up a lot of the error throwing code.
	Actually, it already existed in Dao.Object.Data as 'simpleError1', so I changed it's name to
	'objectError'.

* Make sure evaluation is fully completed before signalling the jobCompletion semaphore.
	As long as there are no deadlocks in any other threads, or errors halting threads leaving MVars
	emptied, then the threads should pause and resume in the order I intented without printing
	information after the prompt is printed. This problem went away without me doing much more than
	simply working out the other bugs. However, for good measure I placed a 'yield' statement just
	after the command which waits for the JobQueue to empty in the 'startRuntimeLoop' function of
	"Dao.Runtime".

* Loading some binary trees cause a deadlock. Check effects of loading bad data.
	In Dao/Runtime.hs:execInputString, a call is made to Dao/Object/Evaluator.hs:runExecScript. If
	this call raised an exception, the thread executing it would not remove itself from the job
	table, and the job table would deadlock waiting for the MVar to be updated. Solved the problem
	by wrapping the call to 'runExecScript' in a 'try' statement, then removing the evaluating
	thread from the job table before re-throwing the exception caught by 'try'.

* Need a better pretty-printer for 'Dao.Object's.
	"Dao.Object.Show" module now has very simple pretty-printing functions that prints objects
	using the same syntax as how they are parsed. 
	-- Done.

* Pattern matching algorithm is returning actions for only the shortest pattern matches. So a
	pattern like "aaa bbb" will mask patterns like "aaa bbb ccc", such that typing "aaa bbb ccc"
	will not return the associated actions, even if that pattern is explicitly defined.
	--
	Solved by add just one more case matching statement to the loop, the case where a LeafBranch is
	encountered and the list of remaining unmatched tokens is NOT empty will return the Leaf part as
	a result, and will loop again over the branch part using the remaining tokens. If only all my
	problems were this easy to solve.

* Binary tree files not loading.
	--
	First problem: the Dao/Document.hs:openDoc function would decode a binary file inside a
	"modifyMVar_" statement, and return a value of a type inferred by the contents of the MVar. The
	data file only contained a value of type 'Document DocData'. However the MVar contained a value
	of type 'Map Name (Document DocData)'. Thus the type inference would result in the wrong
	'Data.Binary.decode' function to be called, decoding the file as though it were of type
	'Map Name (Document DocData)', when the contents of the file was not of this type, hence the
	decoder always failed when it checked for the Document magic number.
	--
	Second problem: the data stored is not retrievable by the lookup function in Dao/Tree.hs:lookup.
	When printing the contents of a loaded idea file, it appears to be empty, even though it is
	clearly not empty when you look at the raw binary data.
	Second problem: I checked how a tree object was being stored. It turns out, I was using
	the Dao.Tree data type's instantiation of the Data.Binary class, and this instantiation relies
	on the instantiation of the tree's leafs and branches into the Data.Binary class as well.
	Now, the 'Document's root object was data type of type 'Tree UStr Object', so the
	instantiation of UStr and Object into the Data.Binary class were both used to store the tree.
	The 'Tree' data type stores it's branches by storing a list of pairs using my own efficient list
	storage algorithm, where each pair is simply the concatenation of a UStr and an Object, every
	list element is concatenated, and the list is null-terminated. This algorithm therefore assumes
	that there is a non-null prefix to UStr and Object, particularly UStr. When a list of such pairs
	is read, it first checks if the next byte is null, and if it is, the list is done being read.
	Unfortunately, the default instantiation of UStr DID NOT prefix it's data appropriately, and a
	64-bit big-endian number containing the length of the string was the only prefix for the UStr,
	meaning strings of length less than 2^(64-8) would have a null prefix. Thus, the tree would fail
	to read on the very first branch. I solved this problem by simply modifying UStr's instantiation
	into the Data.Binary class to prefix every string with a 0x01 byte.

* Built-in function calls that "return" cause the calling function to return.
	--
	Created the "catchCEReturn" function to convert functions evaluating to CEReturn to functions
	evaluating to CENext, while leaving alone functions that evaluate to CENext or CEError.

* Modify the 'DocList doc' data type to contain a (Map Name (MVar (Document doc))) type. Then,
	modify the "with" statement to operate directly on the MVar, rather than looking up the document
	each time, and the modifying it in the table.
	-- Done.

* When loading files, try parsing an idea file expressed as Dao code, rather than binary. First try
	binary, then try parsing a tree expression.
	--
	Instead of doing this, I am going to fiddle with the language definition to allow for "sub
	rules" and "private"ly imported files that share their static data and rules only with the
	programs that have imported them.

* Need to fill in remaining binary operators.
	-- Done.

* Modify the Runtime system to allow loading "import" files. Imports will be shared by programs that
	import them, and executing a string against a program will execute that string against any rules
	in any of the imported program files as well. If the imported program contains no rules, the
	publicly declared functions can still be called by the rules in the importing program file.
	--
	Actually, all files loaded by the runtime will be "public", so importing files with hidden
	rules may not make a whole lot of sense if they are imported by the runtime. However, we can
	have public and hidden rule sets, hidden sets are loaded into a separate 'ProgramTable' in the
	runtime, and input to the runtime is only matched against rule sets in the public
	'ProgramTable', leaving the programs loaded into the private table to be executed only the
	programs that imported them. So each 'ExecUnit' will have its own 'ProgramTable', but the
	programs in the import table are shared with the programs in the private table to prevent source
	files from being loaded twice.
	Finally, this requires a slight re-design to the 'Dao.Runtime.loadFile' function. This function
	should merely return a file, or a 'Either DocHandle (MVar ExecUnit)', and the
	'newRuntimeWithFiles' function can choose where to place these files, as can a separate
	'loadImports' function (which has not yet been written).
	--
	Done, but may have caused a regression. The "Dao.Files" module was created, and the 'File' data
	type was added to the "Dao.Types" module. Now files are loaded once, and indexed in multiple
	tables. The system-level file path provides the unique ID for the File data type.
	Meta-information indicates whether it is a Document or an executable Dao script. Files are
	indexed publicly if loaded through arguments to the "dao" executable program, or privately if
	loaded by "import" statements in public Dao scripts. Changing a script from "private" (imported)
	to "public" is a matter of moving it's 'File' data value from one table to another. All string
	execution now properly executes against "public" script files, and only the "private" scripts
	loaded by the program which contains it, or some subset of these scripts depending on the
	parameters passed to the "do" function. In the case of user input, strings are only executed
	against "public" scripts.

----------------------------------------------------------------------------------------------------
FEATURE REQUESTS

* Insert a new "OParser" constructor to the Object data type. OParser objects will be able to make
	use of the same facilities Dao uses to parse code to create complex text-matching functions.

* There may be a need to be "sub rules" in a program, or to perhaps have a new "private" keyword,
	such that these rules that can only have strings executed against them when requested to by some
	other rule.
	--
	This is not going to happen. I think allowing for top-level functions to be defined, and
	allowing the user to control which modules an input string is executed against using the "do"
	function, provides enough control over how strings are executed recursively. Adding "sub rules"
	will make things unnecessarily complicated.

* Change the 'Left' value of the inside of the 'CombinationT' monad to store a 'SomeException' type,
	rather than a 'String' type.
	--
	This cannot be done reasonably. The only way for this to be possible is to catch all errors as
	'SomeException'. Further, you cannot pass your own 'Exception's as 'Left' values because the
	type of the Exception will be 'forall'ed away, which breaks referential transparency and is
	illegal in Haskell. You will get an error like "forall value escapes context" or something to
	that effect. If there is a way to do it without breaking referential transparency, like creating
	one big super-Exception that can construct all possible exceptions in the program, then we can
	consider implementing this change.

* Modify the language definition to include the "+=", "-=", "*=", etc. (the update in-place
	operators).
	--
	NO, this needlessly complicates things. It is requesting that I create a separate set of
	assignment functions for updating local variables and global variables. The type-overloading
	casting operations need to know what assignment operator was used, I need a separate set of
	function apart from the ordinary "+", "-", "*", etc (equation binary operators) that can do a
	lookup-and-update rather than simply returning the computed value. I could modify the language
	definition such that the expression:
		a += 1*b;
	is automatically translated to
		a = a + (1*b);
	but then the expression will show-up different from how it was parsed when it is
	pretty-printed, which is something I am trying to avoid. If I were to implement the feature of
	passing local variable references to functions, then the update in-place operators could be
	implemented as built-in functions, where the left-hand side is always a Local or Global variable
	reference. But I am not going to do that, just write the updating code out by hand, like in the
	Python language.

* Allow passing of local variable references as references to functions.
	--
	NO, This would require making local variables first-class values at some point in the execution.
	The easiest way to do this is to define a new Object value 'Local::Object'. But these types are
	only used at runtime, and this would add bulk to the marshalled binary representation of the
	Object data type. I want to avoid complicating the 'Object' data type, however this seems to be
	the easiest way to implement this feature. The alternative implementation is to modify the
	'Dao.Object.Evaluator.evalObject' function to NOT work with Objects, but to work with some type
	like this:
		data Symbol = Local Name | Global [Name] | Const Object
	So for example, thinking of the "+=" operator as a function, the left-hand-side of the operator
	must be a variable, either Local or Global, and the right hand side must be a Const. This would
	change how argument passing is done for functions calls: it would require some kind of re-naming
	of references if a reference were passed as an argument, which requires re-thinking how the
	ExecUnit{execStack :: MVar [T_dict]} is implemented, for example, changing the container from a
	T_dict (Map Name Object) to a (Map Name (MVar Object)), and copying MVars of re-named variables
	to the contexts of function calls taking Local or Global references as parameters. This is all
	just way too complicated for the purposes of the Dao system.

* Modify the language definition to type check function calls.
	--
	I don't really want to do this. The type system may be OK for simple types, and you could even
	have a "TypeSet" type, like "string|int" to represent a value that can be a string or integer.
	This can get complicated quickly. If we are using types to check function call graphs, then
	functions need to have complex type definitions, like (int->int->pair), and they need to be
	included in the binary specification, and in the function table. Then, should list types be
	allowed to represent the contents, for example list<int>, list<int|string> or
	(list<int>|list<string>), or
		pair<pair<pair<int,string>|pair<list<int>>>,<dict<tree<dict<string>>>>>,int|string|list<string>>
	would that be necessary? OK, suppose we only allow simple types, and combinations of simple
	types like "int|list|string|dict", and only functions are allowed multiple type parameters, and
	perhaps maybe a C-style "typedef" mechanism to label more complex types, this will still
	increase the complexity of the language by a very large degree. Will all that added complexity
	really help improve the quality of the Dao code, or will it just thrown in mandatory type
	checking everywhere instead of allowing a Dao programmer to just write their own checks where
	they needed them? Are we really releaving the burden of type-checking off the programmer when
	the language is interpreted and not compiled, why not just let the program run and fail and
	then catch the errors? Isn't this good enough type checking for an interpreted language? Isn't
	having no compile time type-checking better for a light-weight language like Dao?

* Create a "pre-execution" phase of program loading, which essentially executes every 'ScriptExpr'
	without applying state changes simply to determine if any of the 'ScriptExpr's evaluate to a
	CEError type. This pre-execution phase may want to substitue a pure State monad for the IO
	monad, and operate only obj object types, rather than actual objects.
	--
	At present, this would only create more work for the compiling phase. If I can determine that
	pre-evaluating will improve efficiency of execution, I will implement this feature.

